{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOtznRTgKSeX"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers albumentations opencv-python tqdm\n",
        "!pip install torchmetrics\n",
        "\n",
        "# %% Mount Google Drive to access your dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassJaccardIndex\n",
        "from transformers import SegformerForSemanticSegmentation\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Global variables for image size & training parameters\n",
        "IMG_HEIGHT = 512   # Change as needed, depending on your dataset resolution\n",
        "IMG_WIDTH = 512\n",
        "BATCH_SIZE = 4     # Adjust depending on Colab free tier memory\n",
        "NUM_EPOCHS = 30    # i want to change them to 25 and re train\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Define label mappings (modify if you have more classes)\n",
        "id2label = {0: \"background\", 1: \"road\"}\n",
        "label2id = {\"background\": 0, \"road\": 1}\n",
        "NUM_CLASSES = len(id2label)\n"
      ],
      "metadata": {
        "id": "uzYVkHueKZyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define offline augmentation pipeline\n",
        "offline_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "])\n",
        "\n",
        "def augment_and_save(images_dir, masks_dir, save_images_dir, save_masks_dir, augmentations_per_image=3):\n",
        "    os.makedirs(save_images_dir, exist_ok=True)\n",
        "    os.makedirs(save_masks_dir, exist_ok=True)\n",
        "\n",
        "    image_paths = sorted(glob(os.path.join(images_dir, \"*\")))\n",
        "    mask_paths = sorted(glob(os.path.join(masks_dir, \"*\")))\n",
        "\n",
        "    for img_path, mask_path in tqdm(zip(image_paths, mask_paths), total=len(image_paths)):\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "        # Save original image and mask\n",
        "        cv2.imwrite(os.path.join(save_images_dir, f\"{base_name}.png\"), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "        cv2.imwrite(os.path.join(save_masks_dir, f\"{base_name}.png\"), mask)\n",
        "\n",
        "        for i in range(augmentations_per_image):\n",
        "            augmented = offline_transform(image=image, mask=mask)\n",
        "            aug_image = augmented['image']\n",
        "            aug_mask = augmented['mask']\n",
        "\n",
        "            aug_image_bgr = cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR)\n",
        "            cv2.imwrite(os.path.join(save_images_dir, f\"{base_name}_aug_{i}.png\"), aug_image_bgr)\n",
        "            cv2.imwrite(os.path.join(save_masks_dir, f\"{base_name}_aug_{i}.png\"), aug_mask)\n",
        "\n",
        "\n",
        "images_dir = '/content/drive/MyDrive/Road Segmentation/training/images'\n",
        "masks_dir = '/content/drive/MyDrive/Road Segmentation/training/groundtruth'\n",
        "save_images_dir = '/content/drive/MyDrive/Road Segmentation/training_augmented/images'\n",
        "save_masks_dir = '/content/drive/MyDrive/Road Segmentation/training_augmented/groundtruth'\n",
        "\n",
        "if not os.path.exists(save_images_dir) or len(os.listdir(save_images_dir)) == 0:\n",
        "    augment_and_save(images_dir, masks_dir, save_images_dir, save_masks_dir, augmentations_per_image=10)\n"
      ],
      "metadata": {
        "id": "Kf8qi0PEKbwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#on the fly transformations\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.GaussNoise(p=0.2),  # Add some noise augmentation\n",
        "    A.OneOf([\n",
        "        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
        "        A.GridDistortion(p=0.5),\n",
        "        A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=0.5),\n",
        "    ], p=0.3),  # Geometric distortions can help with road segmentation\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.3),  # Color variations\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "f6-dS3TJZH7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoadSegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_paths = sorted(glob(os.path.join(images_dir, \"*\")))\n",
        "        self.masks_paths = sorted(glob(os.path.join(masks_dir, \"*\")))\n",
        "        self.transform = transform\n",
        "\n",
        "        assert len(self.images_paths) == len(self.masks_paths), \"Mismatch between images and masks count.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(self.images_paths[idx])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(self.masks_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = np.where(mask > 127, 1, 0).astype(np.uint8)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask.clone().detach().long()"
      ],
      "metadata": {
        "id": "1W0hZWJaKhKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the paths to point to your augmented dataset\n",
        "TRAIN_IMAGES_DIR = '/content/drive/MyDrive/Road Segmentation/training_augmented/images'\n",
        "TRAIN_MASKS_DIR = '/content/drive/MyDrive/Road Segmentation/training_augmented/groundtruth'\n",
        "\n",
        "# Create the full training dataset using augmented images and ground truth masks\n",
        "full_dataset = RoadSegmentationDataset(TRAIN_IMAGES_DIR, TRAIN_MASKS_DIR, transform=train_transform)\n",
        "\n",
        "# Split the dataset into training and validation sets (e.g., 80/20 split)\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "indices = list(range(len(full_dataset)))\n",
        "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "val_dataset = Subset(full_dataset, val_indices)\n",
        "\n",
        "# Create data loaders for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "W4WkxD46KiVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained SegFormer model\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    \"nvidia/segformer-b2-finetuned-ade-512-512\",  # B2 is larger than B0\n",
        "    num_labels=NUM_CLASSES,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k_5KgM7RKnOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "B5RBJUuJKqJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#history dict to store metrics per epoch\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'train_f1': [],\n",
        "    'val_f1': [],\n",
        "    'train_iou': [],\n",
        "    'val_iou': [],\n",
        "    'train_dice': [],\n",
        "    'val_dice': []\n",
        "}"
      ],
      "metadata": {
        "id": "pBXsM1Q0WvZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_dice_score(pred, target, num_classes=2, smooth=1e-6):\n",
        "    dice_scores = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls).float()\n",
        "        target_cls = (target == cls).float()\n",
        "        intersection = torch.sum(pred_cls * target_cls)\n",
        "        union = torch.sum(pred_cls) + torch.sum(target_cls)\n",
        "        dice = (2. * intersection + smooth) / (union + smooth)\n",
        "        dice_scores.append(dice.item())\n",
        "    return sum(dice_scores) / len(dice_scores)\n",
        "\n",
        "# Modified train and validation functions to compute accuracy, F1, IoU, and Dice\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    acc_metric = MulticlassAccuracy(num_classes=NUM_CLASSES).to(device)\n",
        "    f1_metric = MulticlassF1Score(num_classes=NUM_CLASSES).to(device)\n",
        "    iou_metric = MulticlassJaccardIndex(num_classes=NUM_CLASSES).to(device)\n",
        "    dice_total = 0.0\n",
        "\n",
        "    for images, masks in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(pixel_values=images).logits\n",
        "        outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # Update metrics\n",
        "        acc_metric.update(preds, masks)\n",
        "        f1_metric.update(preds, masks)\n",
        "        iou_metric.update(preds, masks)\n",
        "        dice_total += calculate_dice_score(preds, masks, NUM_CLASSES) * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = acc_metric.compute().item()\n",
        "    epoch_f1 = f1_metric.compute().item()\n",
        "    epoch_iou = iou_metric.compute().item()\n",
        "    epoch_dice = dice_total / len(loader.dataset)\n",
        "    return epoch_loss, epoch_acc, epoch_f1, epoch_iou, epoch_dice\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    acc_metric = MulticlassAccuracy(num_classes=NUM_CLASSES).to(device)\n",
        "    f1_metric = MulticlassF1Score(num_classes=NUM_CLASSES).to(device)\n",
        "    iou_metric = MulticlassJaccardIndex(num_classes=NUM_CLASSES).to(device)\n",
        "    dice_total = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(pixel_values=images).logits\n",
        "            outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "            loss = criterion(outputs, masks)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            acc_metric.update(preds, masks)\n",
        "            f1_metric.update(preds, masks)\n",
        "            iou_metric.update(preds, masks)\n",
        "            dice_total += calculate_dice_score(preds, masks, NUM_CLASSES) * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = acc_metric.compute().item()\n",
        "    epoch_f1 = f1_metric.compute().item()\n",
        "    epoch_iou = iou_metric.compute().item()\n",
        "    epoch_dice = dice_total / len(loader.dataset)\n",
        "    return epoch_loss, epoch_acc, epoch_f1, epoch_iou, epoch_dice"
      ],
      "metadata": {
        "id": "UdJciCUjKsuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 7\n",
        "early_stopping_counter = 0\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience, verbose=True)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    train_loss, train_acc, train_f1, train_iou, train_dice = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc, val_f1, val_iou, val_dice = validate(model, val_loader, criterion, device)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Store metrics in history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['train_f1'].append(train_f1)\n",
        "    history['val_f1'].append(val_f1)\n",
        "    history['train_iou'].append(train_iou)\n",
        "    history['val_iou'].append(val_iou)\n",
        "    history['train_dice'].append(train_dice)\n",
        "    history['val_dice'].append(val_dice)\n",
        "\n",
        "    # Logging\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    print(f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n",
        "    print(f\"Train IoU: {train_iou:.4f} | Val IoU: {val_iou:.4f}\")\n",
        "    print(f\"Train Dice: {train_dice:.4f} | Val Dice: {val_dice:.4f}\")\n",
        "\n",
        "    # Early stopping and checkpointing\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/Road Segmentation/best_segformer_model.pt\")\n",
        "        print(\"Saved best model\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        print(f\"Early stopping counter: {early_stopping_counter}/{patience}\")\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break"
      ],
      "metadata": {
        "id": "2JKmqGEdKrXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metric(name, ylabel):\n",
        "    plt.figure()\n",
        "    plt.plot(history[f'train_{name}'], label=f'Train {name.capitalize()}')\n",
        "    plt.plot(history[f'val_{name}'], label=f'Val {name.capitalize()}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    plt.title(f'{ylabel} over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "plot_metric('loss', 'Loss')\n",
        "plot_metric('acc', 'Accuracy')\n",
        "plot_metric('f1', 'F1 Score')\n",
        "plot_metric('iou', 'IoU')\n",
        "plot_metric('dice', 'Dice Score')"
      ],
      "metadata": {
        "id": "zQb54XNjYsyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def decode_segmentation(mask):\n",
        "    # Simple color mapping: background as black, road as white.\n",
        "    colors = {\n",
        "        0: [0, 0, 0],       # background\n",
        "        1: [255, 255, 255]  # road\n",
        "    }\n",
        "    h, w = mask.shape\n",
        "    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for label, color in colors.items():\n",
        "        rgb[mask == label] = color\n",
        "    return rgb\n",
        "\n",
        "# Access the underlying dataset\n",
        "if hasattr(train_dataset, 'dataset'):\n",
        "    original_dataset = train_dataset.dataset\n",
        "else:\n",
        "    original_dataset = train_dataset\n",
        "\n",
        "# Retrieve the image path using the first index in the subset\n",
        "sample_image_path = original_dataset.images_paths[train_dataset.indices[0]]\n",
        "sample_image = cv2.imread(sample_image_path)\n",
        "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
        "orig_h, orig_w, _ = sample_image.shape\n",
        "\n",
        "# Preprocess the image with the same transforms used in validation (without ToTensorV2 conversion)\n",
        "aug = val_transform(image=sample_image)\n",
        "input_tensor = aug[\"image\"].unsqueeze(0).to(device)\n",
        "\n",
        "# Model inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(pixel_values=input_tensor).logits  # shape: [1, num_labels, H, W]\n",
        "prediction = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Resize prediction to original size if needed\n",
        "prediction_resized = cv2.resize(prediction.astype(np.uint8), (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(sample_image)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(decode_segmentation(prediction_resized))\n",
        "plt.title(\"Predicted Road Segmentation\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yPwFyYKYKzME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test‐time inference + single‐sample visualization\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the test images directory and the directory to save predictions\n",
        "TEST_IMAGES_DIR = '/content/drive/MyDrive/Road Segmentation/test_set_images'\n",
        "PREDICTIONS_DIR = '/content/drive/MyDrive/Road Segmentation/test_predictions'\n",
        "\n",
        "# Create the predictions directory if it doesn't exist\n",
        "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Get all image paths from the test directory and its subdirectories\n",
        "test_image_paths = sorted(glob(os.path.join(TEST_IMAGES_DIR, '**', '*.*'), recursive=True))\n",
        "\n",
        "# Loop through each test image and save mask\n",
        "for img_path in tqdm(test_image_paths, desc=\"Running Inference on Test Set\"):\n",
        "    # Read and preprocess the image\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        print(f\"Failed to read image: {img_path}\")\n",
        "        continue\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    orig_h, orig_w, _ = image.shape\n",
        "\n",
        "    # Apply the same validation transforms you used during training\n",
        "    augmented = val_transform(image=image)\n",
        "    input_tensor = augmented[\"image\"].unsqueeze(0).to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(pixel_values=input_tensor).logits\n",
        "        logits = F.interpolate(logits, size=(orig_h, orig_w),\n",
        "                               mode='bilinear', align_corners=False)\n",
        "        prediction = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Convert the prediction to a binary mask (0 background, 255 road)\n",
        "    prediction_mask = (prediction * 255).astype(np.uint8)\n",
        "\n",
        "    # Save the prediction mask (same filename as input)\n",
        "    base_filename = os.path.basename(img_path)\n",
        "    save_path = os.path.join(PREDICTIONS_DIR, base_filename)\n",
        "    cv2.imwrite(save_path, prediction_mask)\n",
        "\n",
        "# Visualize one example\n",
        "\n",
        "# Pick one sample (e.g. the first)\n",
        "sample_img_path  = test_image_paths[0]\n",
        "sample_mask_path = os.path.join(PREDICTIONS_DIR, os.path.basename(sample_img_path))\n",
        "\n",
        "# Load original image and the saved mask\n",
        "orig = cv2.imread(sample_img_path)\n",
        "orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
        "mask = cv2.imread(sample_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Plot them side by side\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(orig)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask, cmap=\"gray\")\n",
        "plt.title(\"Predicted Road Mask\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Er39yIajAqX6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}