{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2EDX78pb7Bn"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers albumentations opencv-python tqdm\n",
        "!pip install torchmetrics\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhsNeuyTcY2K"
      },
      "outputs": [],
      "source": [
        "#necessary imports\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassJaccardIndex\n",
        "from transformers import SegformerForSemanticSegmentation\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from torch.optim import AdamW\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Define label mappings (modify if you have more classes)\n",
        "id2label = {0: \"background\", 1: \"road\"}\n",
        "label2id = {\"background\": 0, \"road\": 1}\n",
        "NUM_CLASSES = len(id2label)\n",
        "\n",
        "NUM_EPOCHS = 15\n",
        "LEARNING_RATE = 5e-5  # Half of original rate, but higher than typical fine-tuning\n",
        "BATCH_SIZE = 8\n",
        "IMG_HEIGHT = 400\n",
        "IMG_WIDTH = 400\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qeJyKqHb-Cc"
      },
      "outputs": [],
      "source": [
        "# Initialize the model architecture\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    \"nvidia/segformer-b2-finetuned-ade-512-512\",\n",
        "    num_labels=NUM_CLASSES,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/Road Segmentation/segformer_model_epochs_25.pt\"))\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aBgMqvJeUTq"
      },
      "outputs": [],
      "source": [
        "#on the fly transformations\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.GaussNoise(p=0.2),  # Add some noise augmentation\n",
        "    A.OneOf([\n",
        "        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
        "        A.GridDistortion(p=0.5),\n",
        "        A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=0.5),\n",
        "    ], p=0.3),  # Geometric distortions can help with road segmentation\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.3),  # Color variations\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_test_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngsDoG2fcqay"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RoadSegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_paths = sorted(glob(os.path.join(images_dir, \"*\")))\n",
        "        self.masks_paths = sorted(glob(os.path.join(masks_dir, \"*\")))\n",
        "        self.transform = transform\n",
        "\n",
        "        assert len(self.images_paths) == len(self.masks_paths), \"Mismatch between images and masks count.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and convert to RGB\n",
        "        image = cv2.imread(self.images_paths[idx])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load mask in grayscale\n",
        "        mask = cv2.imread(self.masks_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = np.where(mask > 127, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY9-CKNgaPtL"
      },
      "outputs": [],
      "source": [
        "\"\"\"import os\n",
        "\n",
        "# Define your directories\n",
        "images_dir = '/content/gdrive/MyDrive/Road Segmentation/images'\n",
        "masks_dir = '/content/gdrive/MyDrive/Road Segmentation/groundtruth'\n",
        "\n",
        "# List all files in each directory\n",
        "image_files = sorted(os.listdir(images_dir))\n",
        "mask_files = sorted(os.listdir(masks_dir))\n",
        "\n",
        "# Normalize filenames by removing extensions\n",
        "image_basenames = set(os.path.splitext(f)[0] for f in image_files)\n",
        "mask_basenames = set(os.path.splitext(f)[0] for f in mask_files)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dibn2eABaSlX"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Images without corresponding masks\n",
        "missing_masks = image_basenames - mask_basenames\n",
        "if missing_masks:\n",
        "    print(\"Images without corresponding masks:\")\n",
        "    for name in missing_masks:\n",
        "        print(f\"{name}\")\n",
        "\n",
        "# Masks without corresponding images\n",
        "missing_images = mask_basenames - image_basenames\n",
        "if missing_images:\n",
        "    print(\"\\nMasks without corresponding images:\")\n",
        "    for name in missing_images:\n",
        "        print(f\"{name}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XDGs2R6cvwK"
      },
      "outputs": [],
      "source": [
        "# Update the paths to point to your dataset\n",
        "IMAGES_DIR = '/content/gdrive/MyDrive/Road Segmentation/images'\n",
        "MASKS_DIR = '/content/gdrive/MyDrive/Road Segmentation/groundtruth'\n",
        "\n",
        "import os\n",
        "\n",
        "file_path = '/content/gdrive/MyDrive/Road Segmentation/groundtruth/boston_2363.png'\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "    print(f\"Deleted: {file_path}\")\n",
        "else:\n",
        "    print(f\"File not found: {file_path}\")\n",
        "\n",
        "# Create the full dataset without transformations\n",
        "full_dataset = RoadSegmentationDataset(IMAGES_DIR, MASKS_DIR, transform=None)\n",
        "\n",
        "# Generate indices for splitting\n",
        "dataset_size = len(full_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
        "val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create subsets with appropriate transformations\n",
        "train_dataset = Subset(RoadSegmentationDataset(IMAGES_DIR, MASKS_DIR, transform=train_transform), train_indices)\n",
        "val_dataset = Subset(RoadSegmentationDataset(IMAGES_DIR, MASKS_DIR, transform=val_test_transform), val_indices)\n",
        "test_dataset = Subset(RoadSegmentationDataset(IMAGES_DIR, MASKS_DIR, transform=val_test_transform), test_indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fifWAVuAhkTd"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCAOittLlYsg"
      },
      "outputs": [],
      "source": [
        "#history dict to store metrics per epoch\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'train_f1': [],\n",
        "    'val_f1': [],\n",
        "    'train_iou': [],\n",
        "    'val_iou': [],\n",
        "    'train_dice': [],\n",
        "    'val_dice': []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fplgG8Ztlh_T"
      },
      "outputs": [],
      "source": [
        "def calculate_dice_score(pred, target, num_classes=2, smooth=1e-6):\n",
        "    dice_scores = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls).float()\n",
        "        target_cls = (target == cls).float()\n",
        "        intersection = torch.sum(pred_cls * target_cls)\n",
        "        union = torch.sum(pred_cls) + torch.sum(target_cls)\n",
        "        dice = (2. * intersection + smooth) / (union + smooth)\n",
        "        dice_scores.append(dice.item())\n",
        "    return sum(dice_scores) / len(dice_scores)\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    acc_metric = MulticlassAccuracy(num_classes=NUM_CLASSES).to(device)\n",
        "    f1_metric = MulticlassF1Score(num_classes=NUM_CLASSES).to(device)\n",
        "    iou_metric = MulticlassJaccardIndex(num_classes=NUM_CLASSES).to(device)\n",
        "    dice_total = 0.0\n",
        "\n",
        "    for images, masks in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, masks = images.to(device), masks.to(device, dtype=torch.long) # Change data type to torch.long\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(pixel_values=images).logits\n",
        "        outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # Update metrics\n",
        "        acc_metric.update(preds, masks)\n",
        "        f1_metric.update(preds, masks)\n",
        "        iou_metric.update(preds, masks)\n",
        "        dice_total += calculate_dice_score(preds, masks, NUM_CLASSES) * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = acc_metric.compute().item()\n",
        "    epoch_f1 = f1_metric.compute().item()\n",
        "    epoch_iou = iou_metric.compute().item()\n",
        "    epoch_dice = dice_total / len(loader.dataset)\n",
        "    return epoch_loss, epoch_acc, epoch_f1, epoch_iou, epoch_dice\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    acc_metric = MulticlassAccuracy(num_classes=NUM_CLASSES).to(device)\n",
        "    f1_metric = MulticlassF1Score(num_classes=NUM_CLASSES).to(device)\n",
        "    iou_metric = MulticlassJaccardIndex(num_classes=NUM_CLASSES).to(device)\n",
        "    dice_total = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "            images, masks = images.to(device), masks.to(device, dtype=torch.long) # Change data type to torch.long\n",
        "            outputs = model(pixel_values=images).logits\n",
        "            outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "            loss = criterion(outputs, masks)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            acc_metric.update(preds, masks)\n",
        "            f1_metric.update(preds, masks)\n",
        "            iou_metric.update(preds, masks)\n",
        "            dice_total += calculate_dice_score(preds, masks, NUM_CLASSES) * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = acc_metric.compute().item()\n",
        "    epoch_f1 = f1_metric.compute().item()\n",
        "    epoch_iou = iou_metric.compute().item()\n",
        "    epoch_dice = dice_total / len(loader.dataset)\n",
        "    return epoch_loss, epoch_acc, epoch_f1, epoch_iou, epoch_dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puVAXLMjl2RA"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "early_stopping_counter = 0\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    train_loss, train_acc, train_f1, train_iou, train_dice = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc, val_f1, val_iou, val_dice = validate(model, val_loader, criterion, device)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Store metrics in history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['train_f1'].append(train_f1)\n",
        "    history['val_f1'].append(val_f1)\n",
        "    history['train_iou'].append(train_iou)\n",
        "    history['val_iou'].append(val_iou)\n",
        "    history['train_dice'].append(train_dice)\n",
        "    history['val_dice'].append(val_dice)\n",
        "\n",
        "    # Logging\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    print(f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n",
        "    print(f\"Train IoU: {train_iou:.4f} | Val IoU: {val_iou:.4f}\")\n",
        "    print(f\"Train Dice: {train_dice:.4f} | Val Dice: {val_dice:.4f}\")\n",
        "\n",
        "    # Early stopping and checkpointing\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "        torch.save(model.state_dict(), f\"/content/gdrive/MyDrive/Road Segmentation/srs_finetuned_segformer_model_epochs_{NUM_EPOCHS}.pt\")\n",
        "        print(\"Saved best model\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        print(f\"Early stopping counter: {early_stopping_counter}/{patience}\")\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19DSbrRZmK2g"
      },
      "outputs": [],
      "source": [
        "# Plotting metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metric(name, ylabel):\n",
        "    plt.figure()\n",
        "    plt.plot(history[f'train_{name}'], label=f'Train {name.capitalize()}')\n",
        "    plt.plot(history[f'val_{name}'], label=f'Val {name.capitalize()}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    plt.title(f'{ylabel} over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "plot_metric('loss', 'Loss')\n",
        "plot_metric('acc', 'Accuracy')\n",
        "plot_metric('f1', 'F1 Score')\n",
        "plot_metric('iou', 'IoU')\n",
        "plot_metric('dice', 'Dice Score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME8oc_ammRBm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import cv2\n",
        "\n",
        "# Define the directory to save predictions\n",
        "PREDICTIONS_DIR = '/content/gdrive/MyDrive/Road Segmentation/predictions_for_srs_data'\n",
        "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Loop through each test sample and save the predicted mask\n",
        "for idx, (image, _) in enumerate(tqdm(test_loader, desc=\"Running Inference on Test Set\")):\n",
        "    image = image.to(device)  # Move image to the appropriate device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(image).logits  # Forward pass\n",
        "        # Resize logits to match original image size if necessary\n",
        "        logits = F.interpolate(logits, size=image.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Iterate through batch to handle each image individually\n",
        "        for i in range(logits.shape[0]):\n",
        "            prediction = torch.argmax(logits[i], dim=0).cpu().numpy()  # Get prediction for current image in batch\n",
        "\n",
        "            # Generate a color mask with distinct colors\n",
        "            prediction_mask_colored = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)\n",
        "\n",
        "            # Map classes to colors (add more if needed)\n",
        "            prediction_mask_colored[prediction == 0] = [0, 0, 0]    # Background (Black)\n",
        "            prediction_mask_colored[prediction == 1] = [255, 0, 0]  # Road (Red)\n",
        "\n",
        "            # Save the color mask with a unique filename\n",
        "            save_path = os.path.join(PREDICTIONS_DIR, f\"prediction_{idx}_{i}.png\")  # Add batch index to filename\n",
        "            cv2.imwrite(save_path, prediction_mask_colored)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a_mw2X0Hd5i"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Select a sample index to visualize\n",
        "sample_idx = 0\n",
        "\n",
        "# Retrieve the original image and corresponding prediction\n",
        "orig_image, _ = test_dataset[sample_idx]\n",
        "orig_image_np = orig_image.permute(1, 2, 0).numpy()  # Convert to HWC format for visualization\n",
        "\n",
        "# Load the saved prediction mask, adding batch index 0 to match the save pattern\n",
        "mask_path = os.path.join(PREDICTIONS_DIR, f\"prediction_{sample_idx}_0.png\")  # Update the mask path\n",
        "mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Check if the mask was loaded successfully\n",
        "if mask is None:\n",
        "    print(f\"Error: Could not load mask from {mask_path}. Check if the file exists and is a valid image.\")\n",
        "else:\n",
        "    # Plot the original image and the predicted mask side by side\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(orig_image_np)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask, cmap=\"gray\")\n",
        "    plt.title(\"Predicted Road Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}